# Bittle四足歩行ロボット深層強化学習プロジェクト仕様書

## 1. プロジェクト概要

### 1.1 目的
Petoi社のBittle四足歩行ロボットを深層強化学習（Deep Reinforcement Learning, DRL）を用いて学習させ、幅15cmの通路を1m直進する歩行制御を実現する。シミュレーション環境（PyBullet）で学習したモデルを実世界のBittleロボットに転移し、段ボール場とつるつるの机の両方の環境で安定した歩行を実現する。

### 1.2 目標
- **主要目標**: 幅15cmの通路を1m直進する歩行制御の実現
- **性能目標**: 
  - 転倒しない安定した歩行
  - 10秒以内での1m移動完了
  - 段ボール場とつるつるの机の両環境での動作
- **技術目標**: シミュレーションから実世界への効果的な転移

## 2. ロボット仕様

### 2.1 ハードウェア仕様
- **ロボット**: Petoi Bittle Robot Dog
- **制御ボード**: NyBoard V1
- **自由度**: 9自由度（各脚2関節 + 首1関節）
- **重量**: 約200g
- **サイズ**: 約20cm × 10cm × 8cm
- **バッテリー持続時間**: 約1時間
- **最大速度**: 40mm/s（安全速度）
- **最大荷重**: 1lb（約450g）

### 2.2 アクチュエータ仕様
- **サーボモータ**: 金属製サーボ（Robotics Kit）
- **関節制御**: PWM制御
- **関節限界**: 各関節の可動範囲に制限あり

## 3. システム要件

### 3.1 計算環境
- **CPU**: Intel Xeon @ 2.20GHz (12コア)
- **メモリ**: 47GB RAM
- **GPU**: NVIDIA L4 (23GB VRAM)
- **CUDA**: 12.2対応
- **OS**: Linux (WSL2)

### 3.2 ソフトウェア要件
- **物理シミュレーション**: PyBullet
- **深層学習フレームワーク**: PyTorch
- **強化学習ライブラリ**: Stable-Baselines3
- **ロボット制御**: OpenCat Framework
- **プログラミング言語**: Python 3.8+
- **推論環境**: CPU推論（学習時のみGPU使用）

## 4. シミュレーション環境設計

### 4.1 PyBullet環境設定
- **物理エンジン**: Bullet Physics Engine
- **タイムステップ**: 1/240秒（240Hz）
- **重力**: 9.81 m/s²
- **地面摩擦**: 可変（段ボール場: 0.8, つるつる机: 0.1）

### 4.2 環境モデル
- **ロボットモデル**: Bittle URDF（[AIWintermuteAI/Bittle_URDF](https://github.com/AIWintermuteAI/Bittle_URDF.git)）
- **通路モデル**: 幅15cm、長さ1mの通路
- **障害物**: 通路の境界壁（高さ5cm）
- **床面材質**: 
  - 段ボール場: 高摩擦（μ=0.8）
  - つるつる机: 低摩擦（μ=0.1）

### 4.3 観測空間
- **関節角度**: 9次元（各関節の現在角度）
- **関節角速度**: 9次元（各関節の角速度）
- **IMUデータ**: 6次元（3軸加速度 + 3軸角速度）
- **位置・姿勢**: 6次元（3D位置 + 3D姿勢）
- **目標位置**: 2次元（X, Y座標）
- **総観測次元**: 32次元

### 4.4 行動空間
- **関節目標角度**: 9次元（各関節の目標角度）
- **行動範囲**: 各関節の物理的制限内
- **制御周波数**: 50Hz（シミュレーション内）

## 5. 深層強化学習アルゴリズム

### 5.1 選択アルゴリズム
- **主要アルゴリズム**: PPO (Proximal Policy Optimization)
- **理由**: 
  - 連続制御に適している
  - サンプル効率が良い
  - 安定した学習が可能
  - 実装が比較的簡単

### 5.2 ネットワーク構造
- **Actor Network**: 
  - 入力層: 32次元
  - 隠れ層: [256, 256, 128]
  - 出力層: 9次元（関節目標角度）
  - 活性化関数: ReLU（隠れ層）、Tanh（出力層）

- **Critic Network**:
  - 入力層: 32次元
  - 隠れ層: [256, 256, 128]
  - 出力層: 1次元（状態価値）
  - 活性化関数: ReLU

### 5.3 ハイパーパラメータ
- **学習率**: 3e-4
- **バッチサイズ**: 64
- **エピソード長**: 1000ステップ（約4秒）
- **GAE λ**: 0.95
- **クリップ範囲**: 0.2
- **価値関数損失係数**: 0.5
- **エントロピー係数**: 0.01

## 6. 報酬関数設計

### 6.1 報酬構成
```
総報酬 = 前進報酬 + 安定性報酬 + 効率性報酬 + ペナルティ
```

### 6.2 詳細報酬
- **前進報酬**: 
  - 前進距離に比例: `+10 * Δx`
  - 目標方向への移動: `+5 * cos(θ) * Δx`

- **安定性報酬**:
  - 転倒ペナルティ: `-1000`（転倒時）
  - 姿勢安定性: `+1 * (1 - |pitch| - |roll|)`
  - 通路内維持: `+10`（通路内）、`-50`（通路外）

- **効率性報酬**:
  - 速度報酬: `+5 * v_x`（前進速度）
  - エネルギー効率: `-0.1 * Σ|τ|`（トルクの総和）

- **ペナルティ**:
  - 関節限界違反: `-10`（各関節）
  - 急激な動作: `-1 * |Δτ|`（トルク変化）

## 7. 学習プロトコル

### 7.1 学習段階
1. **初期学習**: 基本的な歩行パターンの学習
2. **環境適応**: 異なる摩擦係数での学習
3. **精度向上**: 直進精度の向上
4. **転移学習**: 実世界データでの微調整

### 7.2 学習設定
- **総エピソード数**: 1,000,000
- **評価頻度**: 10,000エピソード毎
- **保存頻度**: 50,000エピソード毎
- **学習時間**: 約24-48時間（GPU使用）

### 7.3 評価指標
- **成功率**: 1m移動完了率
- **平均時間**: 移動完了までの平均時間
- **転倒率**: エピソード中の転倒回数
- **軌道偏差**: 理想軌道からの平均偏差
- **エネルギー効率**: 移動距離あたりのエネルギー消費

## 8. 実世界転移戦略

### 8.1 制御構成
- **PC集中制御方式**:
  - **PC側**: 深層学習モデルによる推論計算（CPU使用）
  - **通信**: PC → Bittle（USB/Bluetooth）
  - **Bittle側**: 受信した指令を各関節に送信
  - **制御周波数**: 50Hz（20ms間隔）

- **CPU推論の実現性**:
  - **推論時間**: 1-5ms（Intel Xeon 12コア環境）
  - **通信時間**: 1-3ms
  - **制御余裕**: 12-18ms（十分な余裕）
  - **リアルタイム性**: 問題なし

### 8.2 ドメイン適応手法
- **Domain Randomization**: 
  - 摩擦係数のランダム化
  - 質量パラメータの変動
  - センサーノイズの追加

- **Reality Gap対策**:
  - シミュレーション精度の向上
  - 実世界データでの微調整
  - 適応的制御パラメータ調整

### 8.3 実世界実験環境
- **段ボール場**: 
  - 摩擦係数: 0.8
  - 表面の不均一性あり
  - 軽微な凹凸あり

- **つるつる机**:
  - 摩擦係数: 0.1
  - 平坦な表面
  - 反射特性あり

### 8.4 転移学習プロセス
1. **シミュレーション学習**: 基本歩行パターンの習得
2. **実世界データ収集**: 初期制御でのデータ収集
3. **微調整学習**: 実世界データでのパラメータ調整
4. **性能評価**: 両環境での性能評価

## 9. プロジェクト構造

### 9.1 ディレクトリ構成
```
1m_walking_with_DRL/
├── README.md
├── requirements.txt
├── config/
│   ├── bittle_config.yaml
│   ├── training_config.yaml
│   └── env_config.yaml
├── src/
│   ├── __init__.py
│   ├── environment/
│   │   ├── __init__.py
│   │   ├── bittle_env.py
│   │   └── reward_functions.py
│   ├── models/
│   │   ├── __init__.py
│   │   ├── ppo_agent.py
│   │   └── networks.py
│   ├── training/
│   │   ├── __init__.py
│   │   ├── trainer.py
│   │   └── evaluator.py
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── data_utils.py
│   │   └── visualization.py
│   └── real_world/
│       ├── __init__.py
│       ├── bittle_controller.py
│       ├── transfer_learning.py
│       └── inference_engine.py
├── assets/
│   ├── urdf/
│   │   └── bittle.urdf
│   └── meshes/
│       └── obj/
├── data/
│   ├── models/
│   ├── logs/
│   └── experiments/
├── scripts/
│   ├── train.py
│   ├── evaluate.py
│   └── real_world_test.py
└── tests/
    ├── test_environment.py
    ├── test_models.py
    └── test_training.py
```

### 9.2 主要ファイル説明
- **bittle_env.py**: PyBullet環境の実装
- **ppo_agent.py**: PPOアルゴリズムの実装
- **trainer.py**: 学習プロセスの管理
- **bittle_controller.py**: 実世界Bittleの制御（CPU推論）
- **transfer_learning.py**: 転移学習の実装
- **inference_engine.py**: CPU推論エンジンの実装

## 10. 実装スケジュール

### 10.1 フェーズ1: 環境構築（1-2週間）
- PyBullet環境の構築
- Bittle URDFの統合
- 基本環境のテスト

### 10.2 フェーズ2: 学習実装（2-3週間）
- PPOアルゴリズムの実装
- 報酬関数の設計・実装
- 初期学習の実行

### 10.3 フェーズ3: 最適化（2-3週間）
- ハイパーパラメータの調整
- 報酬関数の最適化
- 性能評価・改善

### 10.4 フェーズ4: 実世界転移（2-3週間）
- CPU推論エンジンの実装
- 実世界制御の実装（PC集中制御）
- 転移学習の実行
- 両環境での性能評価

### 10.5 フェーズ5: 最終評価（1週間）
- 総合性能評価
- ドキュメント整備
- 結果の分析・報告

## 11. リスク管理

### 11.1 技術的リスク
- **シミュレーション精度**: 実世界との差異
- **学習収束**: 局所最適解への収束
- **転移性能**: シミュレーションから実世界への転移失敗
- **通信遅延**: PC-Bittle間の通信遅延
- **リアルタイム性**: 制御ループの遅延

### 11.2 対策
- ドメインランダム化の活用
- 複数の学習初期化での実験
- 段階的な転移学習の実装
- CPU推論の最適化（モデル軽量化）
- 低遅延通信プロトコルの使用
- 制御プロセスの高優先度設定

## 12. 成功基準

### 12.1 シミュレーション環境
- 成功率: 90%以上
- 平均移動時間: 8秒以内
- 転倒率: 5%以下

### 12.2 実世界環境
- 成功率: 80%以上
- 平均移動時間: 10秒以内
- 転倒率: 10%以下
- 両環境での安定動作
- 制御遅延: 20ms以内（50Hz制御）
- CPU推論時間: 5ms以内

## 13. 参考文献・リソース

- [Petoi Bittle Robot Dog](https://www.petoi.com/products/petoi-bittle-robot-dog)
- [Bittle URDF Repository](https://github.com/AIWintermuteAI/Bittle_URDF.git)
- [PyBullet Documentation](https://pybullet.org/wordpress/)
- [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- [OpenCat Framework](https://github.com/PetoiCamp/OpenCat)

---

**作成日**: 2025年1月
**バージョン**: 1.0
**作成者**: プロジェクトチーム
